# 思路
下面由微观到宏观的顺序对整个项目的创建过程进行梳理。

## 一、定义棋盘类 `ChessBoard`
棋盘类用于记录玩家的动作，并根据需要返回当前棋盘的 `state_feature_planes`(一个 `C×H×W` 的矩阵)。棋盘类的构造如下：

### Attribute
* `board_len: int`: 棋盘边长
* `current_player: int`: 当前玩家
* `available_actions: list`: 可用的落点
* `state: dict`: 棋盘状态字典，key 为 `action`，value 为 `current_player`
* `previous_action: int, default=None`： 上一步棋的落点，如果没有则为 `None`

### Method
* `clear_board() -> None`: 清空棋盘
* `is_game_over() -> Tuple[bool, int]`: 判断游戏是否结束
  * 如果还没分出胜负，返回 `(False, None)`
  * 如果分出了胜负，返回 `(True, winner)`
  * 如果平局，返回 `(True, None)`
* `do_action(action: int) -> None`: 根据输入的动作更新棋盘和当前玩家
* `get_state_feature_planes() -> Tensor`: 返回棋盘状态特征张量，维度为 `(C, board_len, board_len)`
* `self_play(mcts) -> NamedTuple[pi_list, z_list, state_feature_planes_list]`: 自演一局，返回局内每一个时间步 $t$ 产生的 $\boldsymbol{\pi}_t$ 组成的列表 `pi_list` 、这一局的胜负结果 $z$ 相对于当前玩家的奖赏 $z_t$ 组成的列表 `z_list` 、以及每一个时间步 $t$ 的 `state_feature_planes​` 组成的 `state_feature_planes_list`

## 二、定义节点类 `Node`
节点类用来构造蒙特卡洛树，根据论文中的说法:
> Each node $s$ in the search tree contains edges $(s, a)$ for all legal action $s ∈\mathcal{A}(s)$. Each edge stores a set of statistics:
>
> $\left\{N(s,a), W(s,a), Q(s,a), P(s,a)\right\}$,
>
> where $N(s, a)$ is the visit count, $W(s, a)$ is the total action value, $Q(s, a)$ is the mean action value and $P(s, a)$ is the prior probability of selecting that edge.

节点应该具有以下属性:
## Attribute
* `c_puct: float`: 探索常数 $c_{puct}$
* `prior_prob: float`: 选择节点的先验概率 $P(s,a)$
* `n_visits: int, default=0`: 访问次数 $N(s,a)$
* `Q: float, default=0`: 累计平均奖赏 $Q(s,a)=1/N(s,a) \sum_{s'|s,a\rightarrow s'} V(s')$
* `U: float, default=0`: 根据 PUCT 算法有: $U(s, a) = c_{puct} P(s, a) \sqrt{\sum_b N(s, b)}/\left(1 + N(s,a) \right)$
* `score: float`: 节点评分 `Score = Q(s,a) + U(s,a)`，在**选择**步骤起作用
* `parent: Node, default=None`: 父级节点
* `children: dict, default={}`: 子节点字典，key 为 `action`，`value` 为 `Node`

## Method
* `select() -> Tuple[action, Node]`: 返回 `score` 最大的子节点，同时返回该节点的 `action`
* `expand(action_probs: list, c_puct: float) -> None`: 拓展节点，`action_probs` 的每一个元素为 `(action, prior_prob)` 元组，根据这个创建子节点，`action_probs` 的长度为当前棋盘的可用落点的总数
* `update(value: float) -> None`: 更新节点的访问次数 $N(s,a)$ 、节点的累计平均奖赏 $Q(s,a)$ 、节点的 $U(s,a)$ 和评分 $Score = Q(s,a)+U(s,a)$
* `backup(value: float) -> None`: 反向传播，更新自己和所有父节点的属性

## 三、定义基于随机走棋的蒙特卡洛树搜索类 `RolloutMCTS`
由于定期对处于自我博弈状态的 `Alpha Gobang Zero` 的策略-价值网络进行评估，所以需要一个类作为 `Alpha Gobang Zero` 的对手。通过计算胜率，来决定是否保存当前最佳模型。
### Attribute
* `root: Node`: 根节点
* `n_iters:int, default=1000`: 模拟次数

### Method
* `get_action(state: dict) -> int`: 根据当前局面返回下一步动作
* `set_root(root: Node) -> Node`: 更新根节点为 `root`，`root` 的父节点将被置为 `Node`, 重用 `root` 的子树

## 四、定义策略-价值网络类 `PolicyValueNet`
现在有一个棋局进行到第 $t$ 步，局面为 $s_t$。 策略价值网络输入 $s_t$，输出策略向量 $\boldsymbol{p}_t \in \mathbb{R}^{n^2}$ 和值 $V_t$，其中 $n$ 为棋盘的边长。$\boldsymbol{p} _t$ 和局面 $s_t$ 下的可用落点 `available_actions` 一起构成 `action_probs`，作为叶节点 $s_L$ 的 `expand(action_probs, c_puct)` 的参数 对叶节点进行拓展；$V_t$ 作为叶节点 $s_L$ 的 `backup(value)` 的参数进行反向传播。

### Blocks

#### `ResidualBlock(inchannels=128, out_channels=128)`

残差块，自上（输入）而下（输出）的组成如下：

* `conv1: nn.Conv2d`: 输入通道数为`in_channels`，输出通道数为 `out_channels`,，卷积核大小为 3×3 ，`padding` 为 1 的卷积层 
*  `batch_norm2: nn.BatchNorm2d`: `num_features` 为输入通道数 `C ` 的批量归一化层
*  `nn.ReLU`
* `conv2: nn.Conv2d`: 输入通道数为 `out_channels`,，输出通道数为  `out_channels`，卷积核大小为 3×3 ，步长为 1，`padding` 为 1 的卷积层
* `batch_norm2: nn.BatchNorm2d`：`num_features` 为  `out_channels` 的批量归一化层
* 跳连接，将输入和 `batch_norm` 的输出相加
* `nn.RelU`

#### `PolicyHead(in_channels=128)`

用来计算对数动作概率向量 $\hat{\boldsymbol{p}} = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right)$，$x_i$ 为全连接层的输出，`policy_head` 组成如下：

* `conv: nn.Conv2d`: 输入通道数为 `in_channels`，输出通道数为 2，卷积核大小为 1×1，步长为 1 的卷积层
* `batch_norm: nn.BatchNorm2d`：`num_features` 为 2 的批量归一化层
* `nn.ReLU`
* `fc: nn.Linear`: 输入特征数为 `2*H*W`，输出特征数为 `81` 的全连接层
* `F.log_softmax`: 对数 `softmax` 

#### `ValueHead(in_channels=128)`

用来计算 $V$ ，组成如下：

* `conv: nn.Conv2d`: 输入通道数为 `in_channels`，输出通道数为 1，卷积核大小为 1×1 的卷积层
* `batch_norm: nn.BatchNorm2d`：`num_features` 为 1 的批量归一化层
* `nn.ReLU`
* `fc: nn.Sequencial`: 全连接神经网络，组成如下：
    * `nn.Linear`: 输入特征数为 `H*W`，输出特征数为 128 的全连接层
    * `nn.ReLU`
    * `nn.Linear`: 输入特征数为 128，输出特征数为 1 的全连接层
    * `nn.Tanh`: 将全连接层的输出映射到 [-1, 1]

### Attribute

* `conv: nn.Sequancial`: 卷积块的组成为：
    * `nn.Conv2d`: 输入通道数为`C`，输出通道数为 128,，卷积核大小为 3×3 ，`padding` 为 1 的卷积层 
    *  `nn.BatchNorm2d`: `num_features` 为输入通道数 `C ` 的批量归一化层
    *  `F.ReLU`
* `residues: nn.Sequencial`: 5 个结构相同的的残差块: `ResidueBlock(128, 128)`
* `policy_head: PolicyHead`:  输入通道数为 128
* `value_head: ValueHead`:  输入通道数为 128

## Method

* `forward(X: Tensor) -> tuple`： 返回对数先验概率向量  $\hat{\boldsymbol{p}}$ 和 `value`
* `get_action_probs_and_value(chess_board: ChessBoard) -> Tuple[list, float]`: 返回每一个可行的动作和它对应的先验概率`(action, prior_prob)`组成的列表 `action_probs`，以及 `value`，列表的长度为 `chess_board` 可用落点的总数，小于等于 `self.forward(state_feature_planes)` 返回的 $\hat{\boldsymbol{p}}$ 的长度
* `get_action_probs_and_value_(batch_state_feature_planes) -> Tuple[list, float]`: 返回动作空间 $\mathcal{A}(s)$ 的所有动作和它对应的先验概率`(action, prior_prob)`组成的列表 `action_probs`，以及 `value`，列表的长度为 $\left|\mathcal{A}(s)\right|$ 即棋盘的 `H×W`

## 五、定义基于策略-价值网络的蒙特卡洛搜索树类 `AlphaZeroMCTS`

根据输入的局面 $s_t$ ，得到表示每一个可行动作执行动作的向量 $\boldsymbol{\pi}_t$ ，并依概率返回一个 `action` 的搜索树，其中 $\boldsymbol{\pi}_t$ 的选取依赖于策略-价值网络 `PolicyValueNet` 的输出 $\boldsymbol{p}_t$。

### Attribute

* `policy_value_net: PolicyValueNet`: 策略-价值网络
* `n_iters`: 迭代搜索次数
* `root: Node`: 根节点

### Method

* `get_action(state: dict, return_pi=False) -> Union[int, Tuple]`: 根据当前局面执行蒙特卡洛数搜索，返回 `action` 或 `(action, pi)`，其中 `pi` 的计算公式为：$\pi(a|s)=N(s,a)^{1/\tau}/\sum_b N(s, b)^{1/\tau}$，其中 $\tau$ 为温度，`action` 依概率 $\pi(a|s)$ 从所有可行的 `action` 中选择
    **Note:**  在自我博弈时，根据论文中的描述

    > For the first 30 moves of each game, the temperature is set to $\tau=1$; this selects moves proportionally to their visit count in MCTS, and ensures a diverse set of positions are encountered. For the remainder of the game, an infinitesimal temperature is used, $\tau \rightarrow 0$. Additional exploration is achieved by adding Dirichlet noise to the prior probabilities in the root node $s_0$, specifically $P(s, a) =(1−\varepsilon) p_a + \varepsilon \eta_a$, where $\varepsilon∼Dir(0.03)$ and $\varepsilon=0.25$;

    应该随时间步 $t$ 改变温度 $\tau$ 并在依概率选择 `action` 的时候添加狄利克雷噪声
* `set_root(root: Node) -> Node`: 更新根节点为 `root`，`root` 的父节点将被置为 `Node`, 重用 `root` 的子树

## 六、自我博弈（一局）

1. 清空棋盘，初始化 `pi_list`、`z_list` 和 `state_feature_planes_list`；
2. 将当前的局面 `state` 输入蒙特卡洛树，得到蒙特卡洛树搜索的结果 `action` 和 $\boldsymbol{\pi}$；
3. 根据蒙特卡洛树返回的 `action` 更新棋盘，将棋盘当前的 `state_feature_planes` 、$\boldsymbol{\pi}$ 添加到对应的列表中；
4. 判断游戏是否结束，如果结束，根据当前的玩家计算出 `z_list` ，接着创建 `self_play_data = namedtuple(pi_list=pi_list, z_list=z_list, state_feature_planes_list=state_feature_planes_list)` 并返回；如果还没结束，回到步骤 2；

## 七、训练

1. 设置自我博弈的次数和 `batch_size` ，实例化`ChessBoard`、`PolicyValueNet`、 `AlphaZeroMTCS` 和 `RolloutMTCS`；
2. 进行一局自我博弈，得到 `self_play_data`，对 `self_play_data` 进行旋转和镜像变换来拓展数据集，将 `self_play_data` 中的数据添加到队列中；
3. 如果队列长度大于等于 `batch_size`，就将从队列中拿出 `batch_size` 的数据，喂给策略-价值网络，根据公式  $loss = (z-v)^2-\boldsymbol{\pi}^T \log \boldsymbol{p} + c||\theta||^2$ 进行误差的反向传播，由此完成一次参数 $\theta$ 的更新（别忘记学习率退火）；
4. 如果遇到 `checkpoint`，需要对模型进行一次评估，让 `AlphaZeroMTCS` 和 `RolloutMTCS` 进行对战，计算胜利率；每当胜率达到新高，就保存当前的 策略-价值模型；如果 `RolloutMTCS` 输的太惨，就增加他的 `n_iters`；
5. 如果没有完成所有的自我博弈，回到步骤2，如果完成所有自我博弈或者发生外部中断，则退出循环