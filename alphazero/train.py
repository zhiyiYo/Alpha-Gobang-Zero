# coding:utf-8
import json
import os
import time

import torch
import torch.nn.functional as F
from torch import nn, optim
from torch.optim.lr_scheduler import MultiStepLR
from torch.utils.data import DataLoader

from .alpha_zero_mcts import AlphaZeroMCTS
from .chess_board import ChessBoard
from .policy_value_net import PolicyValueNet
from .self_play_dataset import SelfPlayData, SelfPlayDataSet


def save_model(train_func):
    """ ä¿å­˜æ¨¡å‹ """
    def wrapper(train_pipe_line, *args, **kwargs):
        try:
            train_func(train_pipe_line)
        except:
            os.makedirs('model', exist_ok=True)
            t = time.strftime('%Y-%m-%d_%H-%M-%S',
                              time.localtime(time.time()))
            path = f'model\\last_policy_value_net_{t}.pth'
            train_pipe_line.policy_value_net.eval()
            torch.save(train_pipe_line.policy_value_net, path)
            print(f'ğŸ‰ è®­ç»ƒç»“æŸï¼Œå·²å°†å½“å‰æ¨¡å‹ä¿å­˜åˆ° {os.path.join(os.getcwd(), path)}')
            # ä¿å­˜æ•°æ®
            with open('log\\train_losses.json',  'w', encoding='utf-8') as f:
                json.dump(train_pipe_line.train_losses, f)
    return wrapper


class PolicyValueLoss(nn.Module):
    """ æ ¹æ® self-play äº§ç”Ÿçš„ `z` å’Œ `Ï€` è®¡ç®—è¯¯å·® """

    def __init__(self):
        super().__init__()

    def forward(self, p_hat, pi, value, z):
        """ å‰é¦ˆ

        Parameters
        ----------
        p_hat: Tensor of shape (N, board_len^2)
            å¯¹æ•°åŠ¨ä½œæ¦‚ç‡å‘é‡

        pi: Tensor of shape (N, board_len^2)
            `mcts` äº§ç”Ÿçš„åŠ¨ä½œæ¦‚ç‡å‘é‡

        value: Tensor of shape (N, )
            å¯¹æ¯ä¸ªå±€é¢çš„ä¼°å€¼

        z: Tensor of shape (N, )
            æœ€ç»ˆçš„æ¸¸æˆç»“æœç›¸å¯¹æ¯ä¸€ä¸ªç©å®¶çš„å¥–èµ
        """
        value_loss = F.mse_loss(value, z)
        policy_loss = -torch.sum(pi*p_hat, dim=1).mean()
        loss = value_loss + policy_loss
        return loss


class TrainModel:
    """ è®­ç»ƒæ¨¡å‹ """

    def __init__(self, board_len=9, lr=0.01, n_self_plays=1500, n_mcts_iters=500, n_feature_planes=4, batch_size=32,
                 start_train_size=2000, check_frequency=100, n_test_games=10, c_puct=4, is_use_gpu=True, **kwargs):
        """
        Parameters
        ----------
        board_len: int
            æ£‹ç›˜å¤§å°

        lr: float
            å­¦ä¹ ç‡

        n_self_plays: int
            è‡ªæˆ‘åšå¼ˆæ¸¸æˆå±€æ•°

        n_mcts_iters: int
            è’™ç‰¹å¡æ´›æ ‘æœç´¢æ¬¡æ•°

        n_feature_planes: int
            ç‰¹å¾å¹³é¢ä¸ªæ•°

        batch_size: int
            mini-batch çš„å¤§å°

        start_train_size: int
            å¼€å§‹è®­ç»ƒæ¨¡å‹æ—¶çš„æœ€å°æ•°æ®é›†å°ºå¯¸

        check_frequency: int
            æµ‹è¯•æ¨¡å‹çš„é¢‘ç‡

        n_test_games: int
            æµ‹è¯•æ¨¡å‹æ—¶ä¸å†å²æœ€ä¼˜æ¨¡å‹çš„æ¯”èµ›å±€æ•°

        c_puct: float
            æ¢ç´¢å¸¸æ•°

        is_use_gpu: bool
            æ˜¯å¦ä½¿ç”¨ GPU
        """
        self.c_puct = c_puct
        self.is_use_gpu = is_use_gpu
        self.batch_size = batch_size
        self.n_self_plays = n_self_plays
        self.n_test_games = n_test_games
        self.n_mcts_iters = n_mcts_iters
        self.check_frequency = check_frequency
        self.start_train_size = start_train_size
        self.device = torch.device('cuda:0' if is_use_gpu else 'cpu')
        self.chess_board = ChessBoard(board_len, n_feature_planes)
        # å®ä¾‹åŒ–ç­–ç•¥-ä»·å€¼ç½‘ç»œå’Œè’™ç‰¹å¡æ´›æœç´¢æ ‘
        self.policy_value_net = self.__get_policy_value_net()
        self.mcts = AlphaZeroMCTS(
            self.policy_value_net, c_puct=c_puct, n_iters=n_mcts_iters, is_self_play=True)
        # åˆ›å»ºä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        self.optimizer = optim.Adam(
            self.policy_value_net.parameters(), lr=lr, weight_decay=1e-4)
        self.criterion = PolicyValueLoss()
        self.lr_scheduler = MultiStepLR(self.optimizer, [400, 800], gamma=0.1)
        # å®ä¾‹åŒ–æ•°æ®é›†
        self.dataset = SelfPlayDataSet(board_len)
        # è®°å½•è¯¯å·®
        self.train_losses = self.__load_losses()

    def __self_play(self):
        """ è‡ªæˆ‘åšå¼ˆä¸€å±€

        Returns
        -------
        self_play_data: namedtuple
            è‡ªæˆ‘åšå¼ˆæ•°æ®ï¼Œæœ‰ä»¥ä¸‹ä¸‰ä¸ªæˆå‘˜:
            * `pi_list`: è’™ç‰¹å¡æ´›æ ‘æœç´¢äº§ç”Ÿçš„åŠ¨ä½œæ¦‚ç‡å‘é‡ Ï€ ç»„æˆçš„åˆ—è¡¨
            * `z_list`: ä¸€å±€ä¹‹ä¸­æ¯ä¸ªåŠ¨ä½œçš„ç©å®¶ç›¸å¯¹æœ€åçš„æ¸¸æˆç»“æœçš„å¥–èµåˆ—è¡¨
            * `feature_planes_list`: ä¸€å±€ä¹‹ä¸­æ¯ä¸ªåŠ¨ä½œå¯¹åº”çš„ç‰¹å¾å¹³é¢ç»„æˆçš„åˆ—è¡¨
        """
        # åˆå§‹åŒ–æ£‹ç›˜å’Œæ•°æ®å®¹å™¨
        self.policy_value_net.eval()
        self.chess_board.clear_board()
        pi_list, feature_planes_list, players = [], [], []

        # å¼€å§‹ä¸€å±€æ¸¸æˆ
        while True:
            action, pi = self.mcts.get_action(self.chess_board)
            # ä¿å­˜æ¯ä¸€æ­¥çš„æ•°æ®
            feature_planes_list.append(self.chess_board.get_feature_planes())
            players.append(self.chess_board.current_player)
            pi_list.append(pi)
            self.chess_board.do_action(action)
            # åˆ¤æ–­æ¸¸æˆæ˜¯å¦ç»“æŸ
            is_over, winner = self.chess_board.is_game_over()
            if is_over:
                if winner is not None:
                    z_list = [1 if i == winner else -1 for i in players]
                else:
                    z_list = [0]*len(players)
                break

        # é‡ç½®æ ¹èŠ‚ç‚¹
        self.mcts.reset_root()

        # è¿”å›æ•°æ®
        self_play_data = SelfPlayData(
            pi_list=pi_list, z_list=z_list, feature_planes_list=feature_planes_list)
        return self_play_data

    @save_model
    def train(self):
        """ è®­ç»ƒæ¨¡å‹ """
        for i in range(self.n_self_plays):
            print(f'ğŸ¹ æ­£åœ¨è¿›è¡Œç¬¬ {i+1} å±€è‡ªæˆ‘åšå¼ˆæ¸¸æˆ...')
            self.dataset.append(self.__self_play())

            # å¦‚æœæ•°æ®é›†ä¸­çš„æ•°æ®é‡å¤§äº start_train_size å°±è¿›è¡Œä¸€æ¬¡è®­ç»ƒ
            if len(self.dataset) >= self.start_train_size:
                data_loader = iter(DataLoader(
                    self.dataset, self.batch_size, shuffle=True, drop_last=False))
                print('ğŸ’Š å¼€å§‹è®­ç»ƒ...')

                self.policy_value_net.train()
                # éšæœºé€‰å‡ºä¸€æ‰¹æ•°æ®æ¥è®­ç»ƒï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
                feature_planes, pi, z = next(data_loader)
                feature_planes = feature_planes.to(self.device)
                pi, z = pi.to(self.device), z.to(self.device)
                for _ in range(5):
                    # å‰é¦ˆ
                    p_hat, value = self.policy_value_net(feature_planes)
                    # æ¢¯åº¦æ¸…é›¶
                    self.optimizer.zero_grad()
                    # è®¡ç®—æŸå¤±
                    loss = self.criterion(p_hat, pi, value.flatten(), z)
                    # è¯¯å·®åå‘ä¼ æ’­
                    loss.backward()
                    # æ›´æ–°å‚æ•°
                    self.optimizer.step()
                    # å­¦ä¹ ç‡é€€ç«
                    self.lr_scheduler.step()

                # è®°å½•è¯¯å·®
                self.train_losses.append([i, loss.item()])
                print(f"ğŸš© train_loss = {loss.item():<10.5f}\n")

            # æµ‹è¯•æ¨¡å‹
            if (i+1) % self.check_frequency == 0:
                self.__test_model()

    def __test_model(self):
        """ æµ‹è¯•æ¨¡å‹ """
        model_path = 'model\\best_policy_value_net.pth'
        # å¦‚æœæœ€ä½³æ¨¡å‹ä¸å­˜åœ¨ä¿å­˜å½“å‰æ¨¡å‹ä¸ºæœ€ä½³æ¨¡å‹
        if not os.path.exists(model_path):
            torch.save(self.policy_value_net, model_path)
            return

        # è½½å…¥å†å²æœ€ä¼˜æ¨¡å‹
        best_model = torch.load(model_path)  # type:PolicyValueNet
        best_model.eval()
        best_model.set_device(self.is_use_gpu)
        mcts = AlphaZeroMCTS(best_model, self.c_puct, self.n_mcts_iters)
        self.mcts.set_self_play(False)
        self.policy_value_net.eval()

        # å¼€å§‹æ¯”èµ›
        print('ğŸ©º æ­£åœ¨æµ‹è¯•å½“å‰æ¨¡å‹...')
        n_wins = 0
        for i in range(self.n_test_games):
            self.chess_board.clear_board()
            self.mcts.reset_root()
            mcts.reset_root()
            while True:
                # å½“å‰æ¨¡å‹èµ°ä¸€æ­¥
                is_over, winner = self.__do_mcts_action(self.mcts)
                if is_over:
                    n_wins += int(winner == ChessBoard.BLACK)
                    break
                # å†å²æœ€ä¼˜æ¨¡å‹èµ°ä¸€æ­¥
                is_over, winner = self.__do_mcts_action(mcts)
                if is_over:
                    break

        # å¦‚æœèƒœç‡å¤§äº 55%ï¼Œå°±ä¿å­˜å½“å‰æ¨¡å‹ä¸ºæœ€ä¼˜æ¨¡å‹
        win_prob = n_wins/self.n_test_games
        if win_prob > 0.55:
            torch.save(self.mcts.policy_value_net, model_path)
            print(f'ğŸ¥‡ ä¿å­˜å½“å‰æ¨¡å‹ä¸ºæœ€ä¼˜æ¨¡å‹ï¼Œå½“å‰æ¨¡å‹èƒœç‡ä¸ºï¼š{win_prob:.1%}\n')
        else:
            print(f'ğŸƒ ä¿æŒå†å²æœ€ä¼˜æ¨¡å‹ä¸å˜ï¼Œå½“å‰æ¨¡å‹èƒœç‡ä¸ºï¼š{win_prob:.1%}\n')
        self.mcts.set_self_play(True)

    def __do_mcts_action(self, mcts):
        """ è·å–åŠ¨ä½œ """
        action = mcts.get_action(self.chess_board)
        self.chess_board.do_action(action)
        is_over, winner = self.chess_board.is_game_over()
        return is_over, winner

    def __get_policy_value_net(self):
        """ åˆ›å»ºç­–ç•¥-ä»·å€¼ç½‘ç»œï¼Œå¦‚æœå­˜åœ¨å†å²æœ€ä¼˜æ¨¡å‹åˆ™ç›´æ¥è½½å…¥æœ€ä¼˜æ¨¡å‹ """
        best_model = 'best_policy_value_net.pth'
        history_models = sorted(
            [i for i in os.listdir('model') if i.startswith('last')])
        # ä»å†å²æ¨¡å‹ä¸­é€‰å–æœ€æ–°æ¨¡å‹
        model = history_models[-1] if history_models else best_model
        model = f'model\\{model}'
        if os.path.exists(model):
            print(f'ğŸ’ è½½å…¥æ¨¡å‹ {model} ...\n')
            net = torch.load(model).to(self.device)  # type:PolicyValueNet
            net.set_device(self.is_use_gpu)
        else:
            net = PolicyValueNet(n_feature_planes=self.chess_board.n_feature_planes,
                                 is_use_gpu=self.is_use_gpu).to(self.device)
        return net

    def __load_losses(self):
        """ è½½å…¥å†å²æŸå¤±æ•°æ® """
        path = 'log\\train_losses.json'
        train_losses = []
        if os.path.exists(path):
            with open(path, encoding='utf-8') as f:
                train_losses = json.load(f)
        else:
            os.makedirs('log', exist_ok=True)
        return train_losses
